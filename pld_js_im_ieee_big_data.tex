
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage[pdftex]{graphicx}
\graphicspath{{images/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

% *** MATH PACKAGES ***
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{url}
\usepackage[linesnumbered]{algorithm2e}

\newcommand{\todo}[1]{\texttt{\textcolor{red}{#1}}}

% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\DeclareMathOperator*{\argmin}{arg\!\min}
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Optimization at Scale using the\\ Alternating Direction Method of Multipliers Algorithm}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Peter Lubell-Doughtie and Jon Sondag}
\IEEEauthorblockA{Intent Media\\
New York, USA\\
\{peter.lubell-doughtie, jon.sondag\}@intentmedia.com}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
We implement the Alternating Direction Method of Multipliers (ADMM) algorithm in Apache Hadoop MapReduce.   We then describe details of using the ADMM algorithm to fit a logistic regression model as a subcomponent in a larger modeling pipeline.  We present results of running a distributed logistic regression algorithm and show that it performs well without parameter tuning.  Throughout we emphasize the practical lessons learned while implementing an iterative MapReduce algorithm and the advantages of remaining within the Hadoop ecosystem.
\end{abstract}

\begin{IEEEkeywords}
distributed algorithms; optimization; analytics models
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
When modeling at scale, the algorithms not only need to function correctly, they need to function on large amounts of data in a practical amount of time.  In this context, we approach modeling in a two step process, the first step is to prototype a model on a sample of the dataset, after validating that the model works on this sample, the second step is to run the model on a production-scale dataset.  In many cases production-scale datasets do not fit into the memory available on a single machine and we need to distribute the dataset over a cluster of machines.

The MapReduce programming model \cite{dean2004} allows clients to run computations on large distributed datasets with commodity machines.  Apache Hadoop is a well known, widely used, and broadly supported MapReduce platform which works with the Hadoop Distributed File System (HDFS)  and supports arbitrarily large datasets \cite{white2009}.  We choose to implement the ADMM algorithm using Hadoop in order to benefit from the wealth of existing Hadoop utilities and the community's extensive knowledge.

Regardless of the problems concerning Hadoop's support for iteration \cite{bu2010}, we show that Hadoop can be used to implement an efficient iterative algorithm which is ``good enough'' in the sense of \cite{lin2012}.  That is to say, the benefits of using a reliable platform that easily integrates with our existing toolset outweigh the small increase in time per job.  To support iteration within Hadoop, we write data from external storage to HDFS, this reduces network latency when transferring large amounts of data.  We also use Hadoop's \emph{Input Splits} to persistently partition data between mappers over multiple iterations. 

To efficiently run algorithms on distributed datasets we must use algorithms that can be parallelized.  Our algorithms must be able to compute a partial solution on a subset of the data and then later combine these partial solutions to produce a complete solution.  The ADMM algorithm is a distributed convex optimization algorithm that allows us to fit logistic regression, lasso, L1 and L2 norm, support vector machine, and other popular models.  When executing long-running modeling jobs hand-tuning paramters can be especially time-consuming.  To avoid this, we use ADMM with methods that automatically tune parameters while still guaranteeing convergence.

The model that we prototyped predicted the estimated value $\hat{y}$ as the product of a conditional expectation and a conditional probability:
\begin{equation}
\hat{y} = \mathbb{E}[b|b>0,\mathbf{A}]\Pr(b>0|\mathbf{A}).
\label{eq:formula}
\end{equation}
We fit the expectation, $\mathbb{E}[b|b>0,\mathbf{A}]$, using linear regression and we fit the conditional probability, $\Pr(b>0|\mathbf{A})$, using logistic regression.  This model performed quite well in prototyping, and the next step was to test it on a production-scale dataset.  Fitting a linear regression model is possible using an analytic method, therefore we use matrix algebra to compute the analytic solution and solve for the expectation in Equation \ref{eq:formula}.

However, there is no analytic solution for logistic regression, we must fit the logistic regression model using an approximate method.  Many of the approximate methods traditionally used to fit logistic regression require access to the entirety of the training dataset at once, i.e. they require that the training dataset can be fit into memory.  This require that memory scale with the size of the dataset, and they often cannot be run in parallel.

The ADMM algorithm does not have this property.  The ADMM algorithm is an optimization algorithm suitable for convex optimization, which can be used to compute logistic regression in distributed and parallel manner.   ADMM computes the solution to an intermediate computationally intensive problem on subdivided portions of the training data and then combines these intermediate solutions to find the final solution \cite{boyd}.

The intermediate solutions can be computed in parallel on multiple nodes, this makes the ADMM algorithm amenable to the MapReduce programming model.  On each iteration the ADMM algorithm optimizes the objective function in the parallel distributed fashion described above.  Although the iterations must be run sequentially, the time-consuming computation during each iteration can be parallelized.  This is opposed to other methods where the computation on each iteration is also sequential.  Because of its scalability, we use the ADMM algorithm to fit a logistic regression model and predict the probability in Equation \ref{eq:formula}.

The rest of the paper is organized as follows.  In section \ref{sec:imp} we provide details concerning our implementation of the ADMM algorithm, its application to logistic regression, and how to implement it in Hadoop MapReduce.  In section \ref{sec:results} we present the results of running ADMM on a production scale dataset.  In section \ref{sec:related} we review recent work related to iterative optimization with Hadoop, and in section \ref{sec:conc} we conclude.

\section{Implementation}\label{sec:imp}
Here we describe exactly how we implemented ADMM for Hadoop MapReduce and provide practical details relevant to using it to build models in a production environment.

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{aws_implementation}
\caption{The Job Runner sets paths for the input, the output data, the Jar file with ADMM, and any adjustable parameters; e.g. the number of iterations.  ADMM computes and stores intermediate data in HDFS, then after the final iteration, the $\hat{y}$ values are output to S3.}
\label{fig:workflow}
\end{figure}

\subsection{Logistic Regression with ADMM}
Given a matrix of features $\mathbf{A}\in\mathbf{R}^{m\times n}$ and a vector of their labels $b\in\mathbf{R}^m$ with $b_i\in\{-1,1\}$, our goal is to compute the probability $\text{Pr}(b_i=1|\mathbf{A}_i)$ for each row $1\leq i\leq m$.  The first column of the feature matrix represents the intercept and all values in this column are set to 1.

In general, the convex model fitting problem with regularization can be written as:
\begin{align*}
\text{minimize}&\quad \sum_{i=1}^m l_i(\mathbf{A}_i^Tx - b_i) + r(x),
\end{align*}
where $l_i$ is the loss for example $i$, $r$ is a regularization term, and $x\in\mathbf{R}^n$ is the vector that solves the optimization problem and defines the logistic regression model.

In the case of L2 regularized logistic regression the problem becomes:
\begin{align*}
\text{minimize}&\quad \sum_{i=1}^m \log(-b_i\mathbf{A}_i^Tx) + \lambda\|x\|_2^2,
\end{align*}
where $\lambda$ is the regularization factor.2â€Ž

Rephrasing in ADMM global consensus form and reflecting the fact that the training examples are partitioned across $N$ machines in the cluster, the problem becomes:
\begin{align*}
\text{minimize}&\quad \sum_{i_1=1}^{m_1} \log(-b_{i_1}\mathbf{A}_{i_1}^Tx_{_1})+\ldots \\
&\quad+\sum_{i_N=1}^{m_N} \log(-b_{i_N}\mathbf{A}_{i_N}^Tx_{_N})+\lambda\|z\|_2^2\\
\text{subject to}&\quad x_{_j} - z = 0, j = 1, \ldots, N
\end{align*}
where $x_{_j}\in\mathbf{R}^n$ is the logistic regression fit on machine $j$ and $z\in\mathbf{R}^n$ is a variable representing the global consensus.  After running the algorithm, $z$ will be a vector of parameters that defines the model. The constraint enforces equality of the parameter estimates across machines.

The ADMM algorithm for the above problem is:
\begin{align}
\label{eq:x}
x_{_j}^{k+1} :=&\quad \argmin_{x_{_j}} \sum_{i_j=1}^{m_j} \log(-b_{i_j}\mathbf{A}_{i_j}^Tx_{_j})+ \dfrac{\rho^k}{2}\|x_{_j} - z^k + u_{_j}^k\|_2^2\\
\label{eq:z}
z^{k+1} :=&\quad \begin{cases}
    \bar{x}_q^{k+1} + \bar{u}_q^k& \text{if $q=1$}\\
    \\
    \dfrac{N\rho}{2\lambda + N\rho}(\bar{x}_q^{k+1} + \bar{u}_q^k)& \text{otherwise}
  \end{cases}\\
\nonumber\\
\label{eq:u}
u_j^{k+1} :=&\quad u_j^k + x_j^{k+1} + z^{k+1},
\end{align}
\todo{in above I changed $u_i$ to $u_j$, shouldn't this align with the $x_j$?}
where $k$ is the iteration number, $\rho^k$ is the penalty parameter for iteration $k$, which is used as the step size, and $\bar{x}_q$ represents the average across all mappers of the $q$th element of the vector $\bar{x}\in\mathbf{R}^n$.  Formally, $\bar{x}_q = \sum_{j=1}^N x_{q_j}$, and similarly for $\bar{u}$.  The first elements of $\bar{x}$ and $\bar{u}$, $\bar{x}_1$ and $\bar{u}_1$, correspond to the intercept and are not regularized, therefore we must treat them differently from the other values in the vector.

For each mapper $j$, $\mathbf{A}_{i_j}$ and $b_{i_j}$ are the portion of the data assigned to and used in that mapper.  The values of the $x_{_j}$ are calculated in parallel on the mappers while the values of $z$ and $u$ are calculated on the reducer.  The convex minimization problem for each $x_{_j}$ update is solved using the L-BFGS (low-memory BFGS) method \cite{bonnans2003numerical}.  We use a Java implementation of L-BFGS which is self contained and has performed well on Hadoop nodes.\footnote{\url{https://code.google.com/p/vladium/}}

\subsection{ADMM within MapReduce}
The MapReduce model divides into three phases: the map, the shuffle, and the reduce.  In the mapper phase the input data is split among a set of mappers and each mapper applies a function to the set of data assigned to it.  In the shuffle phase the output of the mappers is assigned to a reducer, and then in the reduce phase the mapper output is aggregated to create the final values output by the reducer.  We may express the map and reduce steps as:
\begin{alignat*}{2}
&\text{map}\quad &(k_1,v_2)\quad\rightarrow &\texttt{list}(k_2,v_2)\\
&\text{reduce}\quad &(k_2,\texttt{list}(v_2))\quad\rightarrow &\texttt{list}(v_2),
\end{alignat*}
where each mapper has a unique key $k_1$.  The shuffle is concerned with mapping the intermediate key-value pairs, $(k_2,v_2)$, output by the mappers to the reducers \cite{dean2004}.

To implement MapReduce in Hadoop we define classes for the mapper, the reducer, and a driver that handles input arguments and specifies the mapper and reducer classes (the shuffle is handled by Hadoop).  In the context of ADMM each mapper performs the resource intensive task of computing the current $x_{_j}$.  After all mappers have completed their computations, we use a single reducer to compute the $z$ and $u_{_j}$ updates.

\subsubsection{Peristent Data with Input Splits}
In the ADMM algorithm, when each mapper calculates the $x_{_j}^{k+1}$ values, on iteration $k+1$, it must use the $u_{_j}^k$ values that were calculated for the same mapper in the previous iteration, $k$.  Hadoop does not normally accommodate this sort of persistence.  \cite{boyd} suggest using Apache HBase, a distributed data store, however this would add a new component and its accompanying complexity to the modeling framework.

As an alternative, we use Hadoop's notion of input splits to keep track of where data is stored and to associate the correct $x_{_j}$ and $u_{_j}$ values with each other.  Input splits specify a \emph{split length} and a \emph{split ID}, which determine the size of the split data and the node on which to perform computations on that data, respectively.  To ensure that the correct $u_{_j}$ values are loaded when calculating an $x_{_j}$ value, the mapper reads the split ID and chooses the $u_i$ values based on this split ID.  The $z$ values are the same in each $x_{_j}$ formula, i.e. across mappers, and we simply replicate the current $z$ values across all mappers at the start of each iteration.

\subsubsection{Automatically Updating $\rho$}

We use the reducer to update the penalty parameter $\rho$.  The number of iterations before convergence depends upon the $\rho$ parameter, and hand-tuning $\rho$ can take a significant amount of time.  This is because we must wait for the algorithm to complete before determining the efficacy of the chosen $\rho$.

By varying the penalty parameter we can reduce the performance impact of our initial choice of $\rho$, and avoid spending time hand-tuning $\rho$.  We use the update scheme suggested in \cite{boyd} to update $\rho$ on each iteration:
\begin{equation}
\rho^{k+1}:=\begin{cases}
  \tau^{\text{incr}}\rho^k&\quad \text{if $\|r^k\|_2>\mu\|s^k\|_2$}\\
  \rho^k/\tau^{\text{decr}}&\quad \text{if $\|s^k\|_2>\mu\|r^k\|_2$}\\
  \rho^k&\quad \text{otherwise,}
\end{cases}
\label{eq:r}
\end{equation}
where $r^k$ is the primal residual, $s^k$ is the dual residual, and $\mu>1$, $\tau^{\text{incr}}>1$, and $\tau^{\text{decr}}>1$ are user-specified parameters.  We have used the typical values of $\mu=10$ and $\tau^{\text{incr}}=\tau^{\text{decr}}=2$.

\subsubsection{Considerations for MapReduce}
Algorithm \ref{alg:admm} shows the coordination algorithm for executing the ADMM iterations.  After having completed a mapper and reducer cycle, the algorithm will have finished one iteration and control returns to the driver.  One line 3 the ADMM driver checks to see if the maximum number of iterations have passed or if the predictions have converged.  If this condition is false, we distribute the $z$ and $u_i$ to the mappers and begin the next iteration on lines 5 through 13.  If this condition is true, the algorithm is complete and we write the results to persistent storage, a bucket on Amazon's simple storage service (Amazon S3), and exit.

\todo{double check the while condition}
\begin{algorithm}
\SetAlgoLined
\KwData{feature matrix $\mathbf{A}$ and vector of target values $b$}
\KwResult{a vector of predicted weights $z$}
$k=0$\\
$m=\text{[the number of mapper nodes]}$\\
\While{$k<$ maxIterations and $r^k<r^{k-1}$}{\label{alg:while}
  \For{$j=1\to N$}{
    update $x_{_j}^k$ using Eq. \ref{eq:x}
  }
  update $z^k$ using Eq. \ref{eq:z}\\
  \For{$j=1\to N$}{
     update $u_{_j}^k$ using Eq. \ref{eq:u}
  }
  update $\rho^k$ using Eq. \ref{eq:r}\\
  $k \gets k + 1$
}
write $z^k$ to S3.
\label{alg:admm}
\vspace{1em}
\caption{ADMM algorithm implemented for Hadoop MapReduce.}
\end{algorithm}

The input data used in the algorithm is stored in Amazon S3.  To reduce the network latency involved in transferring data from Amazon S3 to the MapReduce nodes, we store the data---feature matrix $\mathbf{A}$ and target values $b$---in the local HDFS file system of the nodes.  Because we are running Hadoop on Amazon's Elastic Map Reduce (EMR) service we avoid any data transfer fees.  This workflow is shown in Figure \ref{fig:workflow}.

Wherever feasible, we have made the implementation of the ADMM algorithm generic.  Arguments to the driver allow users to exclude columns from the input data, optionally add an intercept, set the initial $\rho$, set the maximum number of iterations, and set the input and output locations. We have released an implementation of the ADMM algorithm for Hadoop as an open source Java package.\footnote{\url{https://github.com/intentmedia/admm}}

\section{Results}\label{sec:results}
As part of our modeling pipeline, we run the ADMM algorithm daily on nearly a terabyte of data.  As depicted in Figure \ref{fig:workflow}, we load the data and a Jar file containing the ADMM algorithm, from Amazon S3 and into EMR instances.  To evaluate the performance of our implementation of the ADMM algorithm, we examine the output of one day's run to compare the change in loss function per iteration.  We have also examined equivalent results from other days and saw that these results do not vary substantially from day to day.

Figure \ref{fig:iter} shows the difference between the predicted values and the actual values per iteration on a logarithmic scale.  The loss decreases, and therefore the accuracy consistently improves, as more iterations pass.  The steep decline at iteration 8 aligns with a substantial increase in the dual residual (not shown).  \todo{The dual residual represents the [...].} After iteration 8 we see an exponential decay in loss function for the subsequent iterations.

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{iter_rnorm_plot}
\caption{Loss between our models predictions and the actual values per iteration on a logarithmic scale.  We see that the loss decreases dramatically and then decelerates at later iterations.}
\label{fig:iter}
\end{figure}

\section{Related Work}\label{sec:related}
The algorithm and our implementation are based upon \cite{boyd}, which introduced the ADMM algorithm.  \cite{bu2010} presents a more efficient approach to iterative computation in Hadoop with modifications to core Hadoop code, which reduce the time needed to transfer data and initialize MapReduce jobs.  This is not the approach we took, we stay within the Hadoop ecosystem and avoid the ramp-up costs associated with a less well known solution.  Based on our experience MapReduce is "good enough" for ADMM and logistic regression.  We evaluate the cost as described in \cite{lin2012}, and conclude that the reduction in implementation costs gained by using Hadoop outweigh the performance improvements that could be gained by using a less well-known platform. 

\cite{planet} presents a MapReduce implementation of tree ensemble learning and involves a wrapper for MapReduce that coordinates iterations of the underlying algorithm.  Apache Mahout contains an implementation of logistic regression that uses stochastic gradient descent (SGD).\footnote{https://cwiki.apache.org/MAHOUT/logistic-regression.html}  As noted in \cite{agarwal2011}, this SGD implementation is sequential, which substantially limits the performance of the algorithm and will make it difficult to scale.  To address this, \cite{agarwal2011} presents a scalable convex loss system that uses a Hadoop compatible variant of MapReduce.

\section{Conclusion}\label{sec:conc}
We have presented our implementation of the ADMM algorithm for Hadoop MapReduce.  Having an ADMM implementation allows us to more easily move our modeling ideas from prototype to production-scale.  We showed a specific example of using the ADMM algorithm to build a logistic regression model.  Importantly, ADMM is a general optimization algorithm that we can apply to other modeling problems with minimal modification.  Our contribution of a generic distributed optimization algorithm allows those using Hadoop to experiment with many different models at scale.
% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...
%more thanks here


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{pld_js_im_ieee_big_data}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)



% that's all folks
\end{document}


