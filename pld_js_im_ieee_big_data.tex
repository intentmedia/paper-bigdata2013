
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage[pdftex]{graphicx}
\graphicspath{{images/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

% *** MATH PACKAGES ***
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{url}
\usepackage{algorithm2e}

\newcommand{\todo}[1]{\texttt{\textcolor{red}{#1}}}

% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\DeclareMathOperator*{\argmin}{arg\!\min}
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Optimization at Scale with the\\ Alternating Direction Method of Multipliers Algorithm}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Peter Lubell-Doughtie and Jon Sondag}
\IEEEauthorblockA{Intent Media\\
New York, USA\\
\{peter.lubell-doughtie, jon.sondag\}@intentmedia.com}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
We implement the Alternating Direction Method of Multipliers (ADMM) algorithm in Apache Hadoop MapReduce and describe how we use the ADMM algorithm to compute logistic regression as a subcomponent in a larger modeling pipeline.  We present a distributed logistic regression algorithm and implementation that performs well without parameter tuning.  We describe practical lessons learned in implementing an open source ADMM MapReduce algorithm.  Furthermore, we describe the advantages to staying implementing an iterative algorithm for Hadoop and remaining within the Hadoop ecosystem.
\end{abstract}

\begin{IEEEkeywords}
distributed algorithms; optimization; analytics models
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
When modeling at scale the solution not only needs to function correctly, it needs to function on large amounts of data in a reasonable amount of time.  In this context, modeling is often approached in a two step process, the first step is to prototype a solution on a sample of the dataset, and the second step is to run it on a production scale dataset.  In many cases production datasets do not fit into the memory available on a single machine and we need to distribute the dataset over a set of machines.

The MapReduce programming model \cite{dean2004} allows us to work with large distributed datasets using commodity machines.  We use the Apache Hadoop \cite{white2009} implementation of MapReduce which is well known, widely used, and supported by many cloud computing providers.  We provide details of implementing an efficient iterative algorithm in Hadoop and how to overcome the challenges of network latency when transferring large amounts of data between clusters within the Amazon ecosystem. 

When using distributed datasets we must use algorithms that can compute on distributed datasets.  Our algorithms must be able to compute a partial solution on a subset of the data and then later combine these partial solutions to produce a complete solution.  The ADMM algorithm is a distributed convex optimization algorithm that allows us to fit logistic regression, lasso, $\ell1$ and $\ell2$ norm, support vector machine, and other popular models.  We can use it without having to hand-tune parameters, which is especially time-consuming when executing long-running modeling jobs.

The model that we prototyped predicted the estimated value $\hat{y}$ as the expectation of the target value $b$ given that this target value is greater than zero and there is a matrix of features $\mathbf{A}$, multiplied by the probability that the target is greater than zero given the features.  We express this as:
\begin{equation}
\hat{y} = \mathbb{E}[b|b>0,\mathbf{A}]\Pr(b>0|\mathbf{A}),
\label{eq:formula}
\end{equation}
where we fit the expectation, $\mathbb{E}[b|b>0,\mathbf{A}]$, using linear regression and we fit the conditional probability, $\Pr(b>0|\mathbf{A})$, using logistic regression.  This model performed quite well in prototyping, and the next step was to test it on a production scale dataset.  Fitting a linear regression model is possible using an analytic method, therefore we use matrix algebra to compute the analytic solution and predict the expectation in Equation \ref{eq:formula}.

However, there is no analytic solution for logistic regression.  To fit the logistic regression model we must use an approximate method.  Many of the approximate methods traditionally used to fit logistic regression require access to the entirety of the training dataset at once, i.e. they require that the training dataset can be fit into memory.  These methods require that memory scale with the size of the dataset, and they cannot be distributed.

The ADMM algorithm does not have this property.  The ADMM algorithm is an optimization algorithm suitable for convex optimization which can be used to compute logistic regression on subdivided portions of the training data and then combine the calculations on each portion to find a global solution \cite{boyd}.  Because of this the ADMM algorithm is amenable to the MapReduce programming model which distributes computations to multiple nodes.  On each iteration the ADMM algorithm optimizes the objective function for each portion of training data and then aggregates these predictions to find the global solution.  This is the algorithm we will use to fit a logistic regression model and predict the probability in Equation \ref{eq:formula}.

The rest of the paper is organized as follows.  In section \ref{sec:imp} we provide details concerning our implementation of the ADMM algorithm, its application to logistic regression, and how to implement it in Hadoop MapReduce.  In section \ref{sec:results} we present the results of running ADMM on a production scale datasets, which includes the improvement in the objective function.  In section \ref{sec:related} we review recent work related to iterative optimization with Hadoop, and in section \ref{sec:conc} we conclude.

\section{Implementation}\label{sec:imp}
Here we describe exactly how we implemented ADMM for Hadoop MapReduce and provide practical details relevant to using it to build models in a production environment.

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{aws_implementation}
\caption{The Job Runner sets paths for the input, the output data, the Jar file with ADMM, and any adjustable parameters; e.g. the number of iterations.  ADMM computes and stores intermediate data in HDFS, then after the final iteration, the $\hat{y}$ values are output to S3.}
\label{fig:workflow}
\end{figure}

\subsection{Logistic Regression with ADMM}
Given a matrix of features $\mathbf{A}\in\mathbf{R}^{m\times n}$ and a vector of their labels $b\in\mathbf{R}^m$ with $b_i\in\{-1,1\}$, our goal is to compute the probability $\text{Pr}(b_i=1|\mathbf{A}_i)$ for each row $1\leq i\leq m$.  The first column of the feature matrix represents the intercept and all values in this column are set to 1.

In general, the convex model fitting problem with regularization can be written as:
\begin{align*}
\text{minimize}&\quad \sum_{i=1}^m l_i(\mathbf{A}_i^Tx - b_i) + r(x)
\end{align*}
where $l_i$ is the loss for example $i$, $r$ is a regularization term, and $x\in\mathbf{R}^n$ is the vector that solves the optimization problem and defines the logistic regression model.

In the case of L2 regularized logistic regression the problem becomes:
\begin{align*}
\text{minimize}&\quad \sum_{i=1}^m log(-b_i\mathbf{A}_i^Tx) + \lambda\|x\|_2^2
\end{align*}
where $\lambda$ is the regularization factor.

Rephrasing the problem in ADMM global consensus form and reflecting the fact that the training examples are partitioned across $N$ machines in the cluster:
\begin{align*}
\text{minimize}&\quad \sum_{i_1=1}^{m_1} log(-b_{i_1}\mathbf{A}_{i_1}^Tx_{_1})+\ldots \\
&+\sum_{i_N=1}^{m_N} log(-b_{i_N}\mathbf{A}_{i_N}^Tx_{_N})+\lambda\|z\|_2^2\\
\text{subject to}&\quad x_{_j} - z = 0, j = 1, \ldots, N
\end{align*}
where $x_{_j}\in\mathbf{R}^n$ is the logistic regression fit on machine $j$ and $z\in\mathbf{R}^n$ is a variable representing the global consensus.  After running the algorithm, $z$ will be taken as the vector of parameters that defines the model. The constraint enforces equality of the parameter estimates across machines.

The ADMM algorithm for this problem is:
\begin{align}
\label{eq:x}
x_{_j}^{k+1} :=&\quad \argmin_{x_{_j}} \sum_{i_j=1}^{m_j} log(-b_{i_j}\mathbf{A}_{i_j}^Tx_{_j}) + \dfrac{\rho^k}{2}\|x_{_j} - z^k + u_{_j}^k\|_2^2\\
\label{eq:z}
z^{k+1} :=&\quad \begin{cases}
    \bar{x}_q^{k+1} + \bar{u}_q^k& \text{if $q=1$}\\
    \\
    \dfrac{N\rho}{2\lambda + N\rho}(\bar{x}_q^{k+1} + \bar{u}_q^k)& \text{otherwise}
  \end{cases}\\
\nonumber\\
\label{eq:u}
u_i^{k+1} :=&\quad u_i^k + x_i^{k+1} + z^{k+1},
\end{align}

Where $\bar{x}_q$ represents average across all mappers of the $q$th element of the vector $\bar{x}\in\mathbf{R}^n$, or in other words, $\bar{x}_q = \sum_{j=1}^N x_{q_j}$.  $x_1$ corresponds to the intercept and is not regularize so we treat it differently from the other values in the vector.  $k$ is the iteration number, and $\rho^k$ is the penalty parameter for iteration $k$, which is used as the step size.

For each mapper $j$, $\mathbf{A_{i_j}}$ and $b_{i_j}$ are the portion of the data assigned to and used in that mapper.  The values of the $x_{_j}$ are calculated in parallel on the mappers while the values of $z$ and $u$ are calculated on the reducer.

The convex minimization problem for each $x_{_j}$ update is solved using the L-BFGS (low-memory BFGS) method \cite{bonnans2003numerical}.  We use a Java implementation of L-BFGS which is self contained and has performed well on Hadoop nodes.\footnote{\url{https://code.google.com/p/vladium/}}

\subsection{ADMM within MapReduce}
The MapReduce model divides into three phases: the map, the shuffle, and the reduce.  In the mapper phase the input data is split among a set of mappers and each mapper applies a function to the set of data assigned to it.  In the shuffle phase the output of the mappers is assigned to a reducer, and then in the reduce phase the mapper output is aggregated to create the final values.

We may express the map and reduce steps the as:
\begin{alignat*}{2}
&\text{map}\quad &(k_1,v_2)\quad\rightarrow &\texttt{list}(k_2,v_2)\\
&\text{reduce}\quad &(k_2,\texttt{list}(v_2))\quad\rightarrow &\texttt{list}(v_2),
\end{alignat*}
where each mapper has a unique key $k_1$.  The shuffle is concerned with mapping the intermediate key-value pairs, $(k_2,v_2)$, output by the mappers to the reducers \cite{dean2004}.

To implement MapReduce in Hadoop we must define classes for the mapper, the reducer, and a driver that handles input arguments and specifies the mapper and reducer classes (the shuffle is handled by Hadoop).  In the context of ADMM each mapper performs the resource intensive task of computing the current $x_{_j}$.  After all mappers have completed their computations, we use a single reducer to compute the $z$ and $u_{_j}$ updates.

\subsubsection{Peristent Data with Input Splits}
In the ADMM algorithm, when each mapper calculates the $x_{_j}^{k+1}$ values, on iteration $k+1$, it must use the $u_{_j}^k$ values that were calculated for the same mapper in the previous iteration, $k$.  Hadoop does not normally accommodate this sort of persistence.  \cite{boyd} suggest using Apache HBase, a distributed data store, however this would add a new component and its accompanying complexity to the modeling framework.

As an alternative, we use Hadoop's notion of input splits to keep track of where data is stored and to associate the correct $x_{_j}$ and $u_{_j}$ values with each other.  Input splits specify a \emph{split length} and a \emph{split ID}, which determine the size of the split data and the node on which to execute the split, respectively.  To ensure that the correct $u_{_j}$ values are loaded when calculating an $x_{_j}$ value, the mapper reads the split ID and chooses the $u_i$ values based on this split ID.  The $z$ values are the same in each $x_{_j}$ formula, i.e. across mappers, and we simply replicate the current $z$ values across all mappers at the start of each iteration.

\subsubsection{Automatically Updating $\rho$}

We use the reducer to update the penalty parameter $\rho$.  The number of iterations before convergence depends upon the $\rho$ parameter, and hand-tuning $\rho$ can take a significant amount of time.  This is because we must wait for the algorithm to complete before determining the efficacy of the chosen $\rho$.

By varying the penalty parameter we can reduce the performance impact of our initial choice of $\rho$, and avoid spending time hand-tuning $\rho$.  We use the update scheme suggested in \cite{boyd} to update $\rho$ on each iteration:
\begin{equation}
\rho^{k+1}:=\begin{cases}
  \tau^{\text{incr}}\rho^k&\quad \text{if $\|r^k\|_2>\mu\|s^k\|_2$}\\
  \rho^k/\tau^{\text{decr}}&\quad \text{if $\|s^k\|_2>\mu\|r^k\|_2$}\\
  \rho^k&\quad \text{otherwise,}
\end{cases}
\label{eq:r}
\end{equation}
where $r^k$ is the primal residual, $s^k$ is the dual residual, and $\mu>1$, $\tau^{\text{incr}}>1$, and $\tau^{\text{decr}}>1$ are user-specified parameters.  We have used the typical values of $\mu=10$ and $\tau^{\text{incr}}=\tau^{\text{decr}}=2$.

\subsubsection{Considerations for MapReduce}
After having completed a mapper and reducer cycle,  the algorithm will have completed one iteration and control returns to the driver.  In addition to initiating the mapper and reducer tasks, the ADMM driver determines if the algorithm has converged.  If it has not converged we distribute the $z$ and $u_i$ to the mappers and begin the next iteration.  If it has converged we write the results to persistent storage, a bucket on Amazon's simple storage service (Amazon S3), and exit.  This is shown in Algorithm \ref{alg:admm} below.

\begin{algorithm}
\SetAlgoLined
\KwData{feature matrix $\mathbf{A}$ and vector of target values $b$}
\KwResult{a vector of predicted weights $z$}
$k=0$\\
$m=\text{[the number of mapper nodes]}$\\
\While{$k<\text{maxIterations}$ and $\text{curSt} > \text{preSt}$}{
  \For{$i=1\to N$}{
    update $x_{_j}^k$ using Eq. \ref{eq:x}\\
    $i \gets i + 1$
  }
  update $z^k$ using Eq. \ref{eq:z}\\
  \For{$i=1\to N$}{
     update $u_{_j}^k$ using Eq. \ref{eq:u}\\
    $i \gets i + 1$
  }
  update $\rho^k$ using Eq. \ref{eq:r}\\
  $k \gets k + 1$
}
write $z^k$ to S3.
\label{alg:admm}
\vspace{1em}
\caption{ADMM algorithm implemented for Hadoop MapReduce.}
\end{algorithm}

The input data that is used in the algorithm is stored in Amazon S3.  To reduce the network latency involved in transferring data from Amazon S3 to the MapReduce nodes, we store the data---feature matrix $\mathbf{A}$ and target values $b$---in the nodes' local HDFS file systems.  We use Amazon's Elastic Map Reduce (EMR) service to execute Hadoop, this allows us to avoid any data transfer fees.  This is the workflow shown in Figure \ref{fig:workflow}

Wherever feasible we have made the implementation of the ADMM algorithm generic.  Arguments to the driver allow users to exclude columns from the input data, add an intercept, set the initial $\rho$, maximum number of iterations, and the input and output locations. We have released an implementation of the ADMM algorithm for Hadoop as an open source Java package.\footnote{\url{https://github.com/intentmedia/admm}}

\section{Results}\label{sec:results}
We run the ADMM algorithm daily on nearly a terabyte of data.  As depicted in Figure \ref{fig:workflow}, we load data stored in Amazon S3 and a Jar file containing the ADMM algorithm into Amazon EMR instances.  To evaluate the performance we examine the output of one day's run, looking at the change in loss function per iteration.  We examined equivalent results from other days and saw that the results do not vary substantially from day to day.

Figure \ref{fig:iter} shows the difference between the predicted values and the actual values per iteration on a logarithmic scales.  We see that the loss decreases over iterations, and therefore that the accuracy consistently improves as a function of iterations.  The steep decline at iteration 8 aligns with a substantial increase in the dual residual (not shown).  \todo{The dual residual represents the [...].} After iteration 8 we see an exponential decay in loss function for the subsequent iterations.

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{iter_rnorm_plot}
\caption{Loss between our models predictions and the actual values per iteration on a logarithmic scale.  We see that the loss decreases dramatically and then decelerates at later iterations.}
\label{fig:iter}
\end{figure}

\section{Related Work}\label{sec:related}
The algorithm and our implementation are based upon \cite{boyd}, which introduced the ADMM algorithm.  \cite{bu2010} presents a more efficient approach to iterative computation in Hadoop with modifications to the core Hadoop that reduce the costs of transferring data.  This is not the approach we took, we decided to stay within the Hadoop ecosystem and avoid the costs associated with a less well known solution.  Based on our experience MapReduce is "good enough", in the sense of the reduction in implementation costs outweigh potential performance improvements \cite{lin2012}, for ADMM and logistic regression. \cite{planet} presents a MapReduce implementation of tree ensemble learning and involves writer a wrapper to MapReduce that coordinates iterations of the underlying algorithm.

Apache Mahout contains an implementation of logistic regression that uses stochastic gradient descent (SGD).\footnote{https://cwiki.apache.org/MAHOUT/logistic-regression.html}  As noted in \cite{agarwal2011} SGD this implementation is sequential, which substantially limits the performance of the algorithm and will make it difficult to scale.  \cite{agarwal2011} presents a scalable convex loss system that uses a Hadoop compatible variant of MapReduce.

\section{Conclusion}\label{sec:conc}
We have presented our implementation of the ADMM algorithm for Hadoop MapReduce.  Having an ADMM implementation allows us to more easily move our modeling ideas from prototype to production scale.  We showed a specific example of using the ADMM algorithm to build a logistic regression model, however ADMM is a general optimization algorithm that we can apply to other modeling problems.  Having a generic distributed optimization algorithm reduces the risk of experimenting with different models at scale.

% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...
%more thanks here


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{pld_js_im_ieee_big_data}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)



% that's all folks
\end{document}


